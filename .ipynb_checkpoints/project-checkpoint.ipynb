{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11dc0599-df0b-44a1-90f4-93f9ce97d489",
   "metadata": {},
   "source": [
    "<h2>Natural Language processing using Spacy and Python</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51276c0a-2de6-4c28-ab40-f6eef54ff5e8",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) is a field focused on enabling computers to understand and process human language. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c688c9e1-ae94-44e4-8616-4a83ae938efc",
   "metadata": {},
   "source": [
    "<h4>Tokenization in NLP</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a6390-92f0-4924-bb98-0a733e9071e4",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking text into pieces, called tokens, and ignoring characters like punctuation marks (,. “ ‘) and spaces. spaCy's tokenizer takes input in form of unicode text and outputs a sequence of token objects\n",
    "- Types of tokenization\n",
    "  <ul>\n",
    "      <li>Word Tokenization</li>\n",
    "      This method divides text into individual words. It's the most common form of tokenization and works well for languages with clear word boundaries.Example: The sentence \"Natural language processing is fascinating.\" becomes [\"Natural\", \"language\", \"processing\", \"is\", \"fascinating\"].\n",
    "\n",
    "   <li>Sentence Tokenization</li>\n",
    "  This technique splits text into sentences rather than words. It’s useful for tasks that require analyzing the structure of a document.\n",
    "   <li>Character Tokenization</li>\n",
    "   Divides text into smaller units(characters)\n",
    "   eg NLP becomes \"N\", \"L\", \"P\"\n",
    "   <li>Subword Tokenization</li>\n",
    "   This method breaks text into smaller units that are larger than single characters but smaller than full words. It’s useful for handling out-of-vocabulary words in NLP tasks.Eg The word \"unhappiness\" could be tokenized into [\"un\", \"happiness\"].\n",
    "   <li>Custom Tokenization</li>\n",
    "   In some cases, custom rules are created based on specific requirements, such as tokenizing hashtags or identifying domain-specific phrases.\n",
    "  </ul>\n",
    "Here i will only deal with word & sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2acb6d1b-3339-4c62-ab13-11bb2950ab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'learning', 'data', 'science', ',', 'you', 'should', \"n't\", 'get', 'discouraged', '!', '\\n', 'Challenges', 'and', 'setbacks', 'are', \"n't\", 'failures', ',', 'they', \"'re\", 'just', 'part', 'of', 'the', 'journey', '.', 'You', \"'ve\", 'got', 'this', '!']\n"
     ]
    }
   ],
   "source": [
    "# word tokenization\n",
    "from spacy.lang.en import English\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "token_doc = nlp(text)\n",
    "token_list = []\n",
    "for token in token_doc:\n",
    "    token_list.append(token.text)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8515ba-d573-48c0-8795-1481c65566e0",
   "metadata": {},
   "source": [
    "- Notice that `Spacy` produces a list that contains each token(word) as a separate item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62894b9-399f-4c66-902b-a2f3746757c8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cccd090c-bf3b-4599-8d9e-28ae7d65b807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: When learning data science, you shouldn't get discouraged!\n",
      "Sentence 2: \n",
      "Challenges and setbacks aren't failures, they're just part of the journey.\n",
      "Sentence 3: You've got this!\n"
     ]
    }
   ],
   "source": [
    "#sentence tokenization\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "doc = nlp(text)\n",
    "sentence = list(doc.sents)\n",
    "for i, sentence in enumerate(sentence):\n",
    "    print(f\"Sentence {i + 1}: {sentence.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d495ed79-102d-476a-a850-ec81349b0a0b",
   "metadata": {},
   "source": [
    "Again, spaCy has correctly parsed the text into the format we want, this time outputting a list of sentences found in our source text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc4e375-00a9-49f6-a8e8-b7149faff4ee",
   "metadata": {},
   "source": [
    "<h4>Cleaning Text Data: Removing Stopwords</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f83279-eded-4889-9d96-5abfa3e3de44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
