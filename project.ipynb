{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3cfb99-d860-417a-bf4a-4b7a2523e488",
   "metadata": {},
   "source": [
    "<h2>Natural Language processing using Spacy and Python</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51276c0a-2de6-4c28-ab40-f6eef54ff5e8",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) is a field focused on enabling computers to understand and process human language. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c688c9e1-ae94-44e4-8616-4a83ae938efc",
   "metadata": {},
   "source": [
    "<h4>Tokenization in NLP</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a6390-92f0-4924-bb98-0a733e9071e4",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking text into pieces, called tokens, and ignoring characters like punctuation marks (,. “ ‘) and spaces. spaCy's tokenizer takes input in form of unicode text and outputs a sequence of token objects\n",
    "- Types of tokenization\n",
    "  <ul>\n",
    "      <li>Word Tokenization</li>\n",
    "      This method divides text into individual words. It's the most common form of tokenization and works well for languages with clear word boundaries.Example: The sentence \"Natural language processing is fascinating.\" becomes [\"Natural\", \"language\", \"processing\", \"is\", \"fascinating\"].\n",
    "\n",
    "   <li>Sentence Tokenization</li>\n",
    "  This technique splits text into sentences rather than words. It’s useful for tasks that require analyzing the structure of a document.\n",
    "   <li>Character Tokenization</li>\n",
    "   Divides text into smaller units(characters)\n",
    "   eg NLP becomes \"N\", \"L\", \"P\"\n",
    "   <li>Subword Tokenization</li>\n",
    "   This method breaks text into smaller units that are larger than single characters but smaller than full words. It’s useful for handling out-of-vocabulary words in NLP tasks.Eg The word \"unhappiness\" could be tokenized into [\"un\", \"happiness\"].\n",
    "   <li>Custom Tokenization</li>\n",
    "   In some cases, custom rules are created based on specific requirements, such as tokenizing hashtags or identifying domain-specific phrases.\n",
    "  </ul>\n",
    "Here i will only deal with word & sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2acb6d1b-3339-4c62-ab13-11bb2950ab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'learning', 'data', 'science', ',', 'you', 'should', \"n't\", 'get', 'discouraged', '!', '\\n', 'Challenges', 'and', 'setbacks', 'are', \"n't\", 'failures', ',', 'they', \"'re\", 'just', 'part', 'of', 'the', 'journey', '.', 'You', \"'ve\", 'got', 'this', '!']\n"
     ]
    }
   ],
   "source": [
    "# word tokenization\n",
    "from spacy.lang.en import English\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "token_doc = nlp(text)\n",
    "token_list = []\n",
    "for token in token_doc:\n",
    "    token_list.append(token.text)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8515ba-d573-48c0-8795-1481c65566e0",
   "metadata": {},
   "source": [
    "- Notice that `Spacy` produces a list that contains each token(word) as a separate item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62894b9-399f-4c66-902b-a2f3746757c8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cccd090c-bf3b-4599-8d9e-28ae7d65b807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: When learning data science, you shouldn't get discouraged!\n",
      "Sentence 2: \n",
      "Challenges and setbacks aren't failures, they're just part of the journey.\n",
      "Sentence 3: You've got this!\n"
     ]
    }
   ],
   "source": [
    "#sentence tokenization\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "doc = nlp(text)\n",
    "sentence = list(doc.sents)\n",
    "for i, sentence in enumerate(sentence):\n",
    "    print(f\"Sentence {i + 1}: {sentence.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d495ed79-102d-476a-a850-ec81349b0a0b",
   "metadata": {},
   "source": [
    "Again, spaCy has correctly parsed the text into the format we want, this time outputting a list of sentences found in our source text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc4e375-00a9-49f6-a8e8-b7149faff4ee",
   "metadata": {},
   "source": [
    "<h4>Cleaning Text Data: Removing Stopwords</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49719a7c-e344-47e5-9e73-35bbfac7c486",
   "metadata": {},
   "source": [
    "Most text data that we work with is going to contain a lot of words that aren't actually useful to us. These words, called stopwords, are useful in human speech, but they don't have much to contribute to data analysis. Removing stopwords helps us eliminate noise and distraction from our text data, and also speeds up the time analysis takes (since there are fewer words to process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b7e736d-012e-454f-bac0-6c223e951be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words:  326\n",
      "First 10 stop words: ['seem', '‘re', 'during', 'moreover', 'therein', 'into', 'they', 'were', 'call', 'very']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print(\"Number of stop words: \", len(spacy_stopwords))\n",
    "#printing the first ten stop words:\n",
    "print('First 10 stop words:', list(spacy_stopwords)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ea60ee3-f684-4e1f-8faf-e632dd8b5b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered sentence: [learning, data, science, ,, discouraged, !, \n",
      ", Challenges, setbacks, failures, ,, journey, ., got, !]\n"
     ]
    }
   ],
   "source": [
    "#removing stopwords from the data\n",
    "filtered_sent = []\n",
    "doc = nlp(text)\n",
    "\n",
    "#filtering stop words\n",
    "for word in doc:\n",
    "    if word.is_stop == False:\n",
    "        filtered_sent.append(word)\n",
    "print(\"Filtered sentence:\", filtered_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e98edcf-0d06-4934-9614-de8e2402e82c",
   "metadata": {},
   "source": [
    " Removing them has boiled our original text down to just a few words that give us a good idea of what the sentences are discussing: learning data science, and discouraging challenges and setbacks along that journey.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19189f19-a53f-40b3-b3a4-543e2e2286d8",
   "metadata": {},
   "source": [
    "<h4>Lexicon Normalization</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0402f029-e25f-4f84-9bf6-b90c5f70ccbc",
   "metadata": {},
   "source": [
    "Lexicon normalization is another step in the text data cleaning process. In the big picture, normalization converts high dimensional features into low dimensional features which are appropriate for any machine learning model. For our purposes here, we're only going to look at lemmatization, a way of processing words that reduces them to their roots.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5127c3a-e5bb-4982-ab16-6ec6d58437c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run \n",
      "runs \n",
      "running \n",
      "runner \n"
     ]
    }
   ],
   "source": [
    "#Implementing lemmatization - reducing a word to it's root meaning\n",
    "lemma = nlp(\"run runs running runner\")\n",
    "for word in lemma:\n",
    "    print(word.text, word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f63eb5-2d8f-411c-9f19-e35bf8b18b0c",
   "metadata": {},
   "source": [
    "<h4>POS Tagging</h4>\n",
    "A word's part of speech defines its function within a sentence. A noun, for example, identifies an object. An adjective describes an object. A verb describes action. Identifying and tagging each word's part of speech in the context of a sentence is called Part-of-Speech Tagging, or POS Tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2341b4ae-9b02-4b7f-8549-0c54594337a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "sells VERB\n",
      "sea NOUN\n",
      "shells NOUN\n",
      "at ADP\n",
      "the DET\n",
      "sea NOUN\n",
      "shore NOUN\n"
     ]
    }
   ],
   "source": [
    "# post tagging\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "docs = nlp(\"She sells sea shells at the sea shore\")\n",
    "for word in docs:\n",
    "    print(word.text, word.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbce54d-5a85-4c8c-844f-3bf9bd1db7f8",
   "metadata": {},
   "source": [
    "<h4>Entity Detection</h4>\n",
    "Entity detection, also called entity recognition, is a more advanced form of language processing that identifies important elements like places, people, organizations, and languages within an input string of text. This is really helpful for quickly extracting information from text, since you can quickly pick out important topics or indentify key sections of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b241bbe-a34e-468b-8c9d-2d5856a22268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(New York City, 'GPE', 384),\n",
       " (Tuesday, 'DATE', 391),\n",
       " (At least 285, 'CARDINAL', 397),\n",
       " (September, 'DATE', 391),\n",
       " (Brooklyn, 'GPE', 384),\n",
       " (Williamsburg, 'GPE', 384),\n",
       " (four, 'CARDINAL', 397),\n",
       " (Zip, 'PERSON', 380),\n",
       " (Bill de Blasio, 'PERSON', 380),\n",
       " (Tuesday, 'DATE', 391),\n",
       " (Orthodox, 'NORP', 381),\n",
       " (Jews, 'NORP', 381),\n",
       " (as young as 6 months old, 'DATE', 391),\n",
       " (up to $1,000, 'MONEY', 394)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy import displacy \n",
    "nytimes= nlp(u\"\"\"New York City on Tuesday declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest national flash point over refusals to inoculate against dangerous diseases.\n",
    "\n",
    "At least 285 people have contracted measles in the city since September, mostly in Brooklyn’s Williamsburg neighborhood. The order covers four Zip codes there, Mayor Bill de Blasio (D) said Tuesday.\n",
    "\n",
    "The mandate orders all unvaccinated people in the area, including a concentration of Orthodox Jews, to receive inoculations, including for children as young as 6 months old. Anyone who resists could be fined up to $1,000.\"\"\")\n",
    "entities = [(i, i.label_, i.label) for i in nytimes.ents]\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb39708-ee92-4ff0-92cb-69aef34d2c32",
   "metadata": {},
   "source": [
    "Using displaCy we can also visualize our input text, with each identified entity highlighted by color and labeled. We'll use style = \"ent\" to tell displaCy that we want to visualize entities here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "662bc542-586a-4d90-a9c4-e9564958ed6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    New York City\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tuesday\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest national flash point over refusals to inoculate against dangerous diseases.<br><br>\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    At least 285\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " people have contracted measles in the city since \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    September\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", mostly in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Brooklyn\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "’s \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Williamsburg\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " neighborhood. The order covers \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    four\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Zip\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " codes there, Mayor \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bill de Blasio\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " (D) said \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tuesday\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".<br><br>The mandate orders all unvaccinated people in the area, including a concentration of \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Orthodox\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Jews\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       ", to receive inoculations, including for children \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    as young as 6 months old\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". Anyone who resists could be fined \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    up to $1,000\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(nytimes,style=\"ent\", jupyter= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a5f4ee-b295-46f1-a798-4f4cb5fee959",
   "metadata": {},
   "source": [
    "<h4>Dependency Parsing</h4>\n",
    "Depenency parsing is a language processing technique that allows us to better determine the meaning of a sentence by analyzing how it's constructed to determine how the individual words relate to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "482e8fce-f20c-430b-b455-0c4bd872476e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lao Zhu Zhu nsubj sought\n",
      "enlightment enlightment dobj sought\n",
      "he he nsubj achieved\n",
      "it it dobj achieved\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Lao Zhu sought enlightment, and he achieved it\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73aa4c3-01dd-46a6-a70c-51bd8eef31c2",
   "metadata": {},
   "source": [
    "This output can be a little bit difficult to follow, but since we've already imported the displaCy visualizer, we can use that to view a dependency diagraram using style = \"dep\" that's much easier to understand:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5e48e66-649b-4516-a1d4-1414e19d4c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"1677d8e853634fd7829b48c6edef2435-0\" class=\"displacy\" width=\"1450\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Lao</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Zhu</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">sought</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">enlightment,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">he</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">achieved</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">it</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1677d8e853634fd7829b48c6edef2435-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1677d8e853634fd7829b48c6edef2435-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1677d8e853634fd7829b48c6edef2435-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1677d8e853634fd7829b48c6edef2435-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1677d8e853634fd7829b48c6edef2435-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1677d8e853634fd7829b48c6edef2435-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1677d8e853634fd7829b48c6edef2435-0-3\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1677d8e853634fd7829b48c6edef2435-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,266.5 L753.0,254.5 737.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1677d8e853634fd7829b48c6edef2435-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1677d8e853634fd7829b48c6edef2435-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1677d8e853634fd7829b48c6edef2435-0-5\" stroke-width=\"2px\" d=\"M420,264.5 C420,2.0 1100.0,2.0 1100.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1677d8e853634fd7829b48c6edef2435-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,266.5 L1108.0,254.5 1092.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1677d8e853634fd7829b48c6edef2435-0-6\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1677d8e853634fd7829b48c6edef2435-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1265.0,266.5 L1273.0,254.5 1257.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57689cb-cfc3-47cb-9fdc-8be06df0bdae",
   "metadata": {},
   "source": [
    "<h4>Word vector Representation</h4>\n",
    "A word vector is a numeric representation of a word that communicates its relationship to other words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65e50cc4-c691-433e-9bff-e9a05c774ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96,)\n",
      "[-1.1861942e+00 -9.1249442e-01 -1.3897794e-01  7.0208502e-01\n",
      "  3.7187719e-01 -5.5389905e-01  3.1117851e-01  6.4828175e-01\n",
      " -6.4046234e-01 -1.0683224e+00 -5.3949989e-02 -9.6142054e-02\n",
      " -3.2191742e-02  7.2896862e-01  1.4663376e-01  1.0728086e+00\n",
      " -1.3139668e+00 -4.3733674e-01  5.3877443e-01 -2.2522332e-01\n",
      " -1.3220045e-01  1.2192971e+00 -7.5704598e-01 -7.1275020e-01\n",
      "  4.0038523e-01  1.6996339e-01  1.0006807e+00 -6.9105172e-01\n",
      " -1.3702026e-01  1.5841055e+00 -8.1059825e-01  1.5627509e-01\n",
      "  2.8953835e-01  1.0808635e-01 -8.2277262e-01 -6.5260947e-02\n",
      " -7.1238708e-01  1.1303772e-01  8.5052550e-01  4.9783963e-01\n",
      "  1.5338853e-01 -1.0406020e+00  4.6463126e-01  1.8587688e-01\n",
      " -1.5972078e-02 -8.1118721e-01 -1.2280668e+00 -2.6232600e-03\n",
      "  2.8735816e-02 -1.3073421e+00  8.2931018e-01  1.1444800e+00\n",
      " -2.0533168e-01 -4.1878861e-01  6.5064961e-01  1.9696614e-01\n",
      "  1.2446871e+00 -1.6359596e-01  4.2302164e-01 -4.2493194e-01\n",
      " -1.0849035e+00  2.3684630e-01  3.6907226e-02  3.7950069e-01\n",
      " -4.1121662e-02  5.2738190e-04  1.4286828e-01 -6.5365630e-01\n",
      " -1.9679287e-01 -3.5301387e-01 -9.1874844e-01 -3.9251328e-01\n",
      "  1.1712306e+00 -6.2709481e-01 -3.2259813e-01 -1.8970196e-01\n",
      " -1.2767859e-01 -4.5519540e-01 -1.6459939e-01 -4.0453270e-01\n",
      " -5.5749977e-01 -7.2686970e-03 -1.7589019e-01 -6.1938727e-01\n",
      "  1.5878568e+00  1.0031846e+00  8.8653660e-01  1.2788743e-02\n",
      "  1.5476339e+00  7.0287842e-01 -9.2280751e-01  4.4938424e-01\n",
      "  2.4245481e-01  1.5861344e-01 -4.0414238e-01  5.9323680e-01]\n"
     ]
    }
   ],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "dopple = nlp(\"Doppleganger\")\n",
    "print(dopple.vector.shape)\n",
    "print(dopple.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cc024c-65e9-4598-a751-2c88b61974ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
